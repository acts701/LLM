{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjRe2Ic4E5bfiFOptmBpne",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acts701/LLM/blob/main/241229_%EA%B0%9C%EB%B0%9C%EC%9D%BC%EC%A7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 첫 번째 모델\n",
        "- AutoModelForSequenceClassification 은 말 그대로 분류모델이지 질문에 답을 하는 문장 검색 서비스에 맞지 않았다.\n",
        "- 이번에 만든 첫 번쨰 모델은 training이 되는구나 정도의 의미를 가질 수 있겠다.\n",
        "- 결과\n",
        "``` bash\n",
        "문맥: 하나님 앞에 나아갈 때는 제물을 잡아야 하고 피를 흐려했다\n",
        "질문: 이들의 제물은 예배가 아니라 뇌물이었다\n",
        "답변: contradiction → neutral로 보는 것이 맞다.\n",
        "```\n",
        "- 고찰\n",
        "    - 답변이 틀린 이유는 label data를 잘못 줘서 학습시켰기 때문이다.\n",
        "    - chat gpt api를 이용해서 text를 읽어 NLI set을 자동으로 만들게 한 것 부터 문제가 있지 않았을까 싶다. 초기 label 자료는 사람이 줘야 하는데 너무 자동화 시켰다.\n",
        "    - 또한 STT 품질의 근원적인 문제가 있어 고민이다."
      ],
      "metadata": {
        "id": "JqWUJ5KJ2wE5"
      }
    }
  ]
}